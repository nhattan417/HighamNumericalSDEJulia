{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Title: Different result using same calculations for Python and Julia.\n",
    "\n",
    "We are having the below code blocks:\n",
    "\n",
    "For Python:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set the seed for reproducibility\n",
    "# np.random.seed(1234)\n",
    "\n",
    "mu = 2\n",
    "sigma = 0.1\n",
    "\n",
    "Xzero = 1\n",
    "\n",
    "T = 1\n",
    "N = 2**9\n",
    "dt = T/N\n",
    "\n",
    "reps = 10**5\n",
    "decreasing_number = 5\n",
    "\n",
    "dW = np.sqrt(dt) * np.random.randn(reps, N)\n",
    "W = np.cumsum(dW, axis=1)\n",
    "\n",
    "Xpath = np.zeros((reps, N+1))\n",
    "Xpath[:, 0] = Xzero\n",
    "\n",
    "Xem = {}\n",
    "\n",
    "for i in range(1, decreasing_number + 1):\n",
    "    Xem[i - 1] = np.zeros((reps, (N//(2**(i-1)))+1))\n",
    "\n",
    "for i in range(N):\n",
    "    Xpath[:, i + 1] = Xpath[:, i] * np.exp((mu - 0.5*sigma**2)*dt + sigma*dW[:, i])\n",
    "\n",
    "for i in range(1, decreasing_number + 1):\n",
    "    Xem[i - 1][:, 0] = Xzero\n",
    "\n",
    "    dWTemp = np.zeros((reps, N//2**(i-1)))\n",
    "\n",
    "    for j in range(2**(i-1)):\n",
    "        dWTemp += dW[:, j::2**(i-1)]\n",
    "\n",
    "    for j in range(N//2**(i-1)):\n",
    "        Xem[i - 1][:, j + 1] = Xem[i - 1][:, j] + mu*Xem[i - 1][:, j]*dt*2**(i-1) + sigma*Xem[i - 1][:, j]*dWTemp[:, j]\n",
    "\n",
    "expectedPathEnd = np.mean(Xpath[:, -1])\n",
    "error = np.zeros(decreasing_number)\n",
    "\n",
    "for i in range(1, decreasing_number + 1):\n",
    "    error[i-1] = abs(np.mean(Xem[i - 1][:, -1]) - expectedPathEnd)\n",
    "\n",
    "log2_error = np.log2(error)\n",
    "\n",
    "print(log2_error)\n",
    "\n",
    "# The result is array([-5.12216237, -4.12863162, -3.14147915, -2.16727841, -1.21713823])\n",
    "```\n",
    "\n",
    "And for Julia:\n",
    "\n",
    "```Julia\n",
    "using Random, CairoMakie, Statistics, Distributions\n",
    "\n",
    "# Random.seed!(1234)\n",
    "\n",
    "mu = 2\n",
    "sigma = 0.1\n",
    "\n",
    "Xzero = 1\n",
    "\n",
    "T = 1\n",
    "N = 2^9\n",
    "dt = T/N\n",
    "\n",
    "reps = 10^3\n",
    "decreasing_number = 5\n",
    "\n",
    "\n",
    "dW = sqrt(dt) * randn(BigFloat, (reps, N))\n",
    "# dW = sqrt(dt) * rand(Uniform(-1, 1), reps, N)\n",
    "# dW = sqrt(dt) * rand(Normal(0, 1), reps, N)\n",
    "W = cumsum(dW, dims = 2)\n",
    "\n",
    "Xpath = zeros(BigFloat, reps, N+1)\n",
    "Xpath[:, 1] .= Xzero\n",
    "\n",
    "Xem = Dict()\n",
    "\n",
    "for i in 1:decreasing_number\n",
    "    Xem[i - 1] = zeros(BigFloat, reps, N÷2^(i-1)+1)\n",
    "end\n",
    "\n",
    "for i in 1:N\n",
    "    Xpath[:, i + 1] .= Xpath[:, i].*exp.((mu .- 0.5*sigma^2)*dt .+ sigma*dW[:, i])\n",
    "end\n",
    "\n",
    "for i in 1:decreasing_number\n",
    "    Xem[i - 1][:, 1] .= Xzero\n",
    "\n",
    "    dWTemp = zeros(reps, N÷2^(i-1))\n",
    "\n",
    "    for j in 0:2^(i-1)-1\n",
    "        dWTemp .+= dW[:, j+1:2^(i-1):end]\n",
    "    end\n",
    "\n",
    "    for j in 1:N÷2^(i-1)\n",
    "        Xem[i - 1][:, j + 1] .= Xem[i - 1][:, j] .+ mu*Xem[i - 1][:, j]*dt*2^(i-1) .+ sigma*Xem[i - 1][:, j].*dWTemp[:, j]\n",
    "    end\n",
    "end\n",
    "\n",
    "expectedPathEnd = mean(Xpath[:, end])\n",
    "error = zeros(BigFloat, decreasing_number)\n",
    "\n",
    "for i in 1:decreasing_number\n",
    "    error[i] = abs(mean(Xem[i - 1][:, end]) - expectedPathEnd)\n",
    "end\n",
    "\n",
    "log2_error = log.(error)\n",
    "\n",
    "print(log2_error)\n",
    "\n",
    "# The result is:\n",
    "# 5-element Vector{BigFloat}:\n",
    "#  -3.554214402096594687743060305349970824588774138976971536723727823461683147052966\n",
    "#  -2.862899579149310574925670462201169384031459470310215567569001669928305681236022\n",
    "#  -2.178214170645856020469139263167143618313205681041395430066430107576287576669499\n",
    "#  -1.501862963526146322712563759336474109500760939648390401298967910116891895820844\n",
    "#  -0.843632167676772655656675830018306695331363346180138261659490194335827273763738\n",
    "```\n",
    "\n",
    "When doing linear regression with the results, Python's result gives the slope of $\\approx 0.977$ and Julia's results gives the slope of $\\approx 0.678$. The case of Julia is not consistent with the theoretical result (slope $1$) whereas Python is. I suspect the problem lies in the random number generator of Julia vs Python?"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
